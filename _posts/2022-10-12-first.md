---
layout: single
title: '신경망을 위한 데이터 표현 & 신경망 최적화'
---

# 1. 신경망을 위한 데이터 표현 📊 {#1-신경망을-위한-데이터-표현-}


## 1.텐서(tensor)란? : 
데이터를 위한 컨테이너 , 텐서는 임의의 차원(축) 개수를 가지는 행렬의 일반화된 모습. 
:::


### 1.1.1 스칼라(랭크 - 0 텐서) {#111-스칼라랭크---0-텐서}

하나의 숫자만 담고 있는 텐서를 스칼라라고 부른다.

넘파이에서 float32나 float64 타입의 숫자 =\> 스칼라 텐서

ndim 속성을 사용해서 넘파이 배열의 축 개수를 확인해보면 스칼라 텐서의
축개수는 0이다. (텐서의 축 개수를 **랭크(rank)**라고 부른다.)
:::


``` {.python}
import numpy as np
x = np.array(12)
print('x의 축의 개수: ',x.ndim)
```


    x의 축의 개수:  0

:::


### 1.1.2 벡터(랭크 - 1 텐서) 

숫자의 배열을 백터 또는 랭크-1 텐서나 1D텐서라고 부른다. 벡터는 1개의
축을 가진다.
:::


``` {.python}
x = np.array([24,6,22,12,5])
print(x)
print('x의 축의 개수: ',x.ndim)
```


    [24  6 22 12  5]
    x의 축의 개수:  1

:::


*위의 벡터는 5차원 벡터이다. 5D벡터 ≠ 5D텐서 ,* **차원 수** *는 특정
축을 따라 놓은 원소 개수 or 텐서의 축의 개수.*

*5D벡터는 하나의 축을 따라 5개의 차원을 가진 벡터이고 , 5D는 5개의 축을 가진 것*
:::


### 1.1.3 행렬(랭크 - 2 텐서)

백터의 배열은 행렬(matrix)또는 랭크-2 텐서나 2D 텐서이다.
:::


``` {.python}
x = np.array([[5,2,72,12,6],
            [21,534,86,3,21],
            [45,12,69,3,4]])
print(x)
print('x의 축의 개수: ',x.ndim)
```

    [[  5   2  72  12   6]
     [ 21 534  86   3  21]
     [ 45  12  69   3   4]]
    x의 축의 개수:  2
:::


위의 벡터에서 첫번째 축의 원소를 행, 두번째 축의 원소를 열이라고 부른다.
:::


### 1.1.4 랭크-3 텐서와 더 높은 랭크의 텐서 

위와 같은 행렬들을 하나의 새로운 배열로 합치면 숫차가 채워진 직육면체로
해석할 수 있는 랭크-3 텐서가 만들어진다.
:::


``` {.python}
x = np.array([[[5,2,72,12,6],
            [21,534,86,3,21],
            [45,12,69,3,4]],
            [[5,2,72,12,6],
            [21,534,86,3,21],
            [45,12,69,3,4]],
            [[5,2,72,12,6],
            [21,534,86,3,21],
            [45,12,69,3,4]]])
print('x의 축의 개수: ',x.ndim)
```


    x의 축의 개수:  3
:::

랭크-3 탠서들을 하나의 배열로 합치면 랭크-4 텐서를 만드는 식으로이어진다.

**딥러닝에서는 보통 랭크 0에서 4까지의 텐서를 다룬다.**
:::


### 1.1.5 핵심속성 

-   축의 개수 (랭크) : 넘파이나 텐서플로 등의 파이썬 라이브러리에서는
    ndim 속성에 저장되어 있다.
-   크기(shape) : 텐서의 각 축을 따라 얼마나 많은 차원이 있는지 나타낸
    파이썬의 튜플이다. 예를 들어 위의 행렬의 크기는 (3,5) , 랭크-3
    텐서는 (3,3,5)이다.
-   테이터 타입(dtype) : 텐서에 포함된 데이터의 타입. ex) float16,
    float32, float64 , uint8 , string 등이 있다.
:::


MNIST 데이터를 활용해서 위의 내용을 구체적으로 다뤄보자.

먼저 MNIST 데이터셋을 불러드린다.



``` {.python}
from tensorflow.keras.datasets import mnist
(train_images,train_labels),(test_images,test_label) = mnist.load_data()
```


    Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz
    11493376/11490434 [==============================] - 0s 0us/step
    11501568/11490434 [==============================] - 0s 0us/step




train_images 배열의 ndim 속성으로 축의 개수를 확인한다.



``` {.python}
train_images.ndim
```


    3



다음으로 배열의 크기를 확인한다.


``` {.python}
train_images.shape
```

    (60000, 28, 28)



dtype 속성을 이용해서 데이터 타입을 확인한다.



``` {.python}
train_images.dtype
```


    dtype('uint8')



위의 내용들로 train_images 배열은 8비트 정수형 , 랭크-3 텐서이고 , 28x28
크기의 정수행렬 6만개가 있는 배열인 것을 알 수 있다.

각 행렬은 흑백 이미지고 , 행렬의 각 원소는 0에서 255사이의 값을 가진다.
:::

## 1.2 넘파이로 텐서 조작하기 

train_image\[i\] 같은 형식으로 배열에 있는 특정 원소들을 선택하는 것을 **슬라이싱(slicing)** 이라고 한다.

다음 코드는 11번째에서 101번째까지 (101번째는 포함X) 숫자를 선택해서
(90,28,28)크기의 배열을 만든다.
:::


``` {.python}
ex_slice = train_images[10:100]
ex_slice.shape
```


    (90, 28, 28)
:::

조금 더 자세한 slicing 표기법은 각 배열의 축을 따라 슬라이싱의 시작
인덱스와 마지막 인덱스를 지정하는 것이다. :(콜론)은 전체 인덱스를
선택한다.
:::


``` {.python}
ex_slice= train_images[10:100,:,:]
print(ex_slice.shape)

ex2_slice = train_images[10:100, 0:25 , 0:25]
print(ex2_slice.shape)
```


    (90, 28, 28)
    (90, 25, 25)



음수 인덱스도 사용할 수 있다. 파이썬 리스트의 음수 인덱스와 마찬가지로
현재 축의 끝에서 상대적인 위치를 나타낸다.

정중앙의 14x14 픽셀 조각을 이미지에서 잘라내려면 다음 코드와 같이 하면
된다.
:::


``` {.python}
mid_slice = train_images[:,7:-7,7:-7]
mid_slice.shape
```


    (60000, 14, 14)



## 1.3 배치 데이터 

딥러닝 모델은 한번에 전체 데이터셋을 처리하지 않는다.

그대신 작은 배치(batch)로 나눈다. 예를 들어 MNIST 숫자 데이터에서 크기가
128인 데이터 배치는 다음과 같다.

``` {.python}
batch = train_images[:128]
```

n번째 배치는 다음과 같다.
:::


``` {.python}
n = 3 
batch = train_images[128 * n : 128 * (n+1)]
```

이런 배치 데이터를 다룰 때 첫번째 축 (0번 축)을 **배치 축** or **배치
차원**이라고 부른다.


## 1.4 텐서의 연산 

:::


### 1.4.1 원소별 연산 


이 연산은 텐서에 있는 각 원소에 독립적으로 적용된다.

넘파이 배열을 다룰 때는 최적화된 넘파인 내장함수로
덧셈,곱셈,뺄셈,relu함수 등을 처리할 수 있다.
:::

``` {.python}
import numpy as np
z = x + y #원소별 덧셈
z = np.maximum(z,0.) #원소별 relu 함수
```



### 1.4.2 브로드캐스팅 {#142-브로드캐스팅}

크기가 다른 두 텐서가 더해질 때 브로드캐스팅이 일어난다.

다음과 같은 순서로 이뤄진다.

1.  큰 텐서의 ndim에 맞춰 작은 텐서에 축이 추가된다.
2.  작은 텐서가 새 축을 따라서 큰 텐서의 크기에 맞도록 반복된다.



### 1.4.3 텐서 곱셈 {#143-텐서-곱셈}

점곱이라고도 불리는 텐서 곱셈은 가장 널리 사용되는 텐서 연산이다.
(*연산자를 사용하는 원소별 곱셈과 다르다*)

넘파이에서 np.dot 함수를 사용해서 텐서 곱셈을 수행한다.



``` {.python}
import numpy as np
x = np.random.random((5,5))
y = np.random.random((5,5))
z = np.dot(x,y)
```

점곱은 임의의 축 개수를 가진 텐서에 일반화 된다.

가장 일반적인 용도인 두 행렬 간의 점곱은 x.shape\[1\] == y.shape\[0\]
일때 점곱 (dot(x,y))이 성립한다.

## 1.5 텐서 크기 변환 {#15-텐서-크기-변환}

텐서 크기 변환은 특정 크기에 맞게 열과 행을 재배열하는 것이다. 물론
텐서의 원소 개수는 동일하다.

``` {.python}
x = np.array([[0,1],
              [2,3],
              [4,5]])
print(x.shape)
x = x.reshape((6,1))
print(x)
x = x.reshape((2,3))
print(x)
```

    (3, 2)
    [[0]
     [1]
     [2]
     [3]
     [4]
     [5]]
    [[0 1 2]
     [3 4 5]]



자주 쓰는 특별한 크기 변환은 **전치(transposition)**이다. 행과 열을
바꾸는 것이다. 예를 들어 , x\[i,j\]는 x\[j,i\]가 된다.

# 2. 그레디언트 기반 최적화 📉 

## 2.1 확률적 경사 하강법 

다음의 순서로 최적화를 진행할 수 있다.

1.  훈련 샘플 배치 x와 이에 상응하는 타깃 y_true를 추출한다.
2.  x로 모델을 훈련하고 예측 y_pred를 구한다(정방향 패스).
3.  이 배치에서 y_pred와 y_true 사이의 오차(모델의 손실)을 계산한다.
4.  모델의 파라미터에 대한 손실 함수의 그레디언트를 계산한다(역방향
    패스).
5.  그레디언트의 반대 방향으로 파라미터를 조금씩 이동시킨다. 예를 들어 W
    -= learning_rate \* gradient처럼 손실을 조금 감소시킨다.

위의 방법이 **미니 배치 SGD**이다.

만약 대상 파라미터가 작은 학습률을 가진 SGD로 최적화되었다면 전역
최솟값이 아닌 지역 최솟값에 갇히게 될 것이다.

이러한 문제를 **모멘텀**이라는 개념으로 해결할 수 있다. 모멘텀은 현재
그레디언트 값 뿐만 아니라 이전에 업데이트한 파라미터에 기초하여 파라미터
**W**를 업데이트 한다.

다음은 모멘텀의 단순 구현 예다.

``` {.python}
past_velocity = 0
momentum = 0.1
while loss > 0.01:
  w, loss , gradient = get_current_parameters()
  velocity = momentum * velocity - learning_rate * gradient
  w = w + momentum * velocity - learning_rate * gradient
  past_velocity = velocity
  update_parameter(w)
```

## 2.2 역전파 알고리즘 {#22-역전파-알고리즘}

복잡한 식의 그레디언트를 계산하는 방법이 **역전파 알고리즘**이다.

연쇄 법칙을 역방향 그래프에 적용하면 , 노드가 연결된 경로를 따라서
우리가 원하는 그레디언트의 계산이 가능해진다.

역전파는 최종 손실값에서 시작해서 맨 위층까지 거꾸로 올라가서 각
파라미터가 손실값에 기여한 정도는 계산한다. 요즘에는 텐서플로와 같이
**자동 미분**이 가능한 프레임 워크 덕분에 역전파를 쉽게 구현할 수 있다.

### 그레디언트 테이프 (GradientTape)

그레디언트 테이프는 해당 코드 블록 안의 모든 텐서 연산을 계산 그래프
형태(**Tape**)로 기록한다. 그 다음 계산 그래프를 사용해서 tf.Variable
클래스 변수 또는 변수 집합에 대한 어떤 출력의 그레디언트도 계산할 수
있다. (tf.Variable은 변경 가능한 상태를 담기 위한 특별한 텐서이다.)

``` {.python}
import tensorflow as tf

x = tf.Variable(0.) # 초기값 0으로 스칼라 변수를 생성
with tf.GradientTape() as tape: 
  y = 2 * x + 3     # 변수에 텐서 연산을 적용
grad_of_y_wrt_x = tape_gradient(y,x) # tape를 사용해서 x에 대한 y의 그레디언트 계산
```
:::
